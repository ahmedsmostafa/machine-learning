{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!conda install -y Cython\n",
    "#!conda install -y -c hcc pycocotools\n",
    "# !pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir /data/models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir, makedirs, path\n",
    "from itertools import islice\n",
    "from pickle import dump, load\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.preprocessing.image import load_img\n",
    "from keras.preprocessing.image import img_to_array\n",
    "from keras.applications.vgg16 import preprocess_input\n",
    "from keras.layers import Input, Reshape, Concatenate\n",
    "import numpy as np\n",
    "import string\n",
    "import collections\n",
    "from progressbar import progressbar\n",
    "from keras.models import Model\n",
    "from pycocotools.coco import COCO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataDir = '/data'\n",
    "annotationsDir = 'annotations'\n",
    "ml_type = 'train'\n",
    "valDataDir = 'val'\n",
    "year = '2014'\n",
    "coco_train_full_path = dataDir + '/' + ml_type + year + '/'\n",
    "load_if_exists=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_coco(ml_type='train', dataset_type='captions', year='2014'):\n",
    "    assert ml_type in ['train', 'test', 'val']\n",
    "    assert dataset_type in ['instances', 'captions']\n",
    "    annFile = path.join(dataDir, path.join(annotationsDir, dataset_type + '_{}.json'.format(ml_type + year)))\n",
    "    print(\"Annotations of {0} {1} Dataset: {2}\".format(ml_type, dataset_type, annFile))\n",
    "    return COCO(annFile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load an image from filepath\n",
    "def load_image(path):\n",
    "    img = load_img(path, target_size=(224, 224))\n",
    "    img = img_to_array(img)\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    img = preprocess_input(img)\n",
    "    return np.asarray(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load doc into memory\n",
    "def load_doc(filename):\n",
    "    # open the file as read only\n",
    "    file = open(filename, 'r')\n",
    "    # read all text\n",
    "    text = file.read()\n",
    "    # close the file\n",
    "    file.close()\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_captions(caption_as_list):\n",
    "    caption_as_list = [word for word in caption_as_list if len(word) > 1]\n",
    "    caption_as_list = [word for word in caption_as_list if word.isalpha()]\n",
    "    return ' '.join(caption_as_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_model(is_attention):\n",
    "    # load the model\n",
    "    model = VGG16()\n",
    "    if is_attention:\n",
    "        # model = VGG16()\n",
    "        model.layers.pop()\n",
    "        # extract final 49x512 conv layer for context vectors\n",
    "        final_conv = Reshape([49, 512])(model.layers[-4].output)\n",
    "        model = Model(inputs=model.inputs, outputs=final_conv)\n",
    "        print(model.summary())\n",
    "    else:\n",
    "        # model = VGG16()\n",
    "        # re-structure the model\n",
    "        model.layers.pop()\n",
    "        model = Model(inputs=model.inputs, outputs=model.layers[-1].output)\n",
    "        print(model.summary())\n",
    "        # extract features from each photo\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def take(n, iterable):\n",
    "    \"Return first n items of the iterable as a list\"\n",
    "    return list(islice(iterable, n))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-50-151f72029f31>, line 7)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-50-151f72029f31>\"\u001b[0;36m, line \u001b[0;32m7\u001b[0m\n\u001b[0;31m    for img_id, name in progressbar(take(limit if limit != -1 else len(all_img_ids), all_img_ids.items())):\u001b[0m\n\u001b[0m                                                                                                          ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "def get_features_from_coco(coco, directory, model, limit):\n",
    "    features = dict()\n",
    "    # counter = 1\n",
    "    print('coco is {}, directory is {}, model is {}, limit is {}'.format(coco, directory, model, limit))\n",
    "    all_img_ids = {key:value[\"file_name\"] for key,value in coco.imgs.items()}\n",
    "    print('all_img_ids is {}'.format(all_img_ids)\n",
    "    for img_id, name in progressbar(take(limit if limit != -1 else len(all_img_ids), all_img_ids.items())):\n",
    "        filename = directory + name\n",
    "        image = load_image(filename)\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        features[img_id] = feature\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features_from_directory(directory, model, limit):\n",
    "    # add a counter to limit size of data\n",
    "    features = dict()\n",
    "    counter = 1\n",
    "    for name in progressbar(listdir(directory)[0:limit if limit != -1 else len(listdir(directory))]):\n",
    "        # ignore README\n",
    "        if not name.endswith('.jpg'):\n",
    "            continue\n",
    "        counter += 1\n",
    "        filename = directory + name\n",
    "        image = load_image(filename)\n",
    "        # extract features\n",
    "        feature = model.predict(image, verbose=0)\n",
    "        # get image id\n",
    "        # image_id = name.split('.')[0]\n",
    "        image_id = name[name.rindex('_') + 1:name.rindex('.jpg')].lstrip(\"0\")\n",
    "        # store feature\n",
    "        features[int(image_id)] = feature\n",
    "        # print('>%s' % name)\n",
    "        if limit != -1 and counter == (limit+1):\n",
    "            break;\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract features from each photo in the coco_train_full_path\n",
    "# this method could be implemented differently:\n",
    "# - load the coco file (annotation or instances)\n",
    "#   - Loop over image IDs (until the limit if not -1):\n",
    "#       - Load the images accordingly\n",
    "#       - Extract features\n",
    "#       - Save into dict\n",
    "# - Save into models folder\n",
    "def extract_features(dataset_dir, is_attention=False, limit=-1,\n",
    "                     load_if_exists=False, models_dir='models/'):\n",
    "    if load_if_exists:\n",
    "        print('loading existing model')\n",
    "        if path.exists(models_dir + '/features.pkl'):\n",
    "            print('found existing features model')\n",
    "            with open(models_dir + '/features.pkl', 'rb') as pickle_file:\n",
    "                features = load(pickle_file)\n",
    "                return features\n",
    "    # load COCO\n",
    "    coco = get_coco(ml_type=ml_type, year=year)\n",
    "    #load model\n",
    "    model = load_model(is_attention)\n",
    "    # features = get_features_from_directory(dataset_dir, model, limit)\n",
    "    features = get_features_from_coco(coco, dataset_dir, model, limit)\n",
    "    # save to file\n",
    "    save_features(features, 'models/')\n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_captions(features, load_if_exists=False, models_dir='models/'):\n",
    "    if load_if_exists:\n",
    "        if path.exists(models_dir + '/captions.pkl'):\n",
    "            with open(models_dir + '/captions.pkl', 'rb') as pickle_file:\n",
    "                captions = load(pickle_file)\n",
    "                return captions\n",
    "    captions = dict()\n",
    "    translator = str.maketrans('', '', string.punctuation)\n",
    "    coco = get_coco(ml_type=ml_type, year=year)\n",
    "    for img_id in progressbar(features):\n",
    "        caps = coco.imgToAnns[img_id]\n",
    "        caps = [cap.pop('caption').strip().lower().translate(translator).split()\n",
    "                for cap in caps]\n",
    "        caps = [clean_captions(cap) for cap in caps]\n",
    "        captions[img_id] = caps\n",
    "    # save to file\n",
    "    save_captions(captions, 'models/')\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert the loaded descriptions into a vocabulary of words\n",
    "def extract_vocabulary(captions):\n",
    "    # build a list of all caption strings\n",
    "    all_desc = set()\n",
    "    for key in captions.keys():\n",
    "        [all_desc.update(d.split()) for d in captions[key]]\n",
    "    return all_desc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save descriptions to file, one per line\n",
    "# format is Key <space> descriptions\n",
    "def save_captions(captions, directory):\n",
    "    lines = list()\n",
    "    for key, desc_list in captions.items():\n",
    "        for desc in desc_list:\n",
    "            lines.append(str(key) + ' ' + desc)\n",
    "    data = '\\n'.join(lines)\n",
    "    file = open(directory+'captions.txt', 'w')\n",
    "    file.write(data)\n",
    "    file.close()\n",
    "\n",
    "    dump(captions, open(directory+'captions.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_features(features, directory):\n",
    "    if not path.exists(directory):\n",
    "        makedirs(directory)\n",
    "    dump(features, open(directory+'features.pkl', 'wb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a 80/20 train/test split\n",
    "def train_test_split(dataset):\n",
    "    # sort the list to keep consistency\n",
    "    # sorted_dataset = collections.OrderedDict(sorted(dataset.items()))\n",
    "    training_count = int(round(0.8*len(dataset)))\n",
    "    training_set = dict(sorted(dataset.items())[:training_count])\n",
    "    testing_set = dict(sorted(dataset.items())[training_count:])\n",
    "    return training_set, testing_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_startend_to_caption(values):\n",
    "    return ['startseq ' + ''.join(value) + ' endseq' for value in values]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_captions(dataset):\n",
    "    return {key:append_startend_to_caption(value) for key,value in dataset.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_clean_captions(filename, dataset):\n",
    "    # load document\n",
    "    doc = load_doc(filename)\n",
    "    captions = dict()\n",
    "    for line in doc.split('\\n'):\n",
    "        # split line by white space\n",
    "        tokens = line.split()\n",
    "        # skip images not in the set\n",
    "        if int(tokens[0]) in dataset:\n",
    "            # split id from caption\n",
    "            image_id, image_cap = tokens[0], tokens[1:]\n",
    "            # create list\n",
    "            if image_id not in captions:\n",
    "                captions[int(image_id)] = list()\n",
    "            # wrap caption in tokens\n",
    "            cap = 'startseq ' + ' '.join(image_cap) + ' endseq'\n",
    "            # store\n",
    "            captions[int(image_id)].append(cap)\n",
    "    return captions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def verify_datasets(images, captions):\n",
    "    counter = 0\n",
    "    not_found = list()\n",
    "    for ikey in images:\n",
    "        if ikey not in captions:\n",
    "            counter += 1\n",
    "            not_found.append(ikey)\n",
    "    for ckey in captions:\n",
    "        if ckey not in images:\n",
    "            counter += 1\n",
    "            not_found.append(ckey)\n",
    "    print(\"{} keys not found when cross-checking images & captions\".format(counter))\n",
    "    print(not_found if len(not_found) > 0 else \"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(models_dir='/models'):\n",
    "    # extract features from all images\n",
    "    print(\"Extracting Features:\")\n",
    "    features = extract_features(coco_train_full_path,\n",
    "                                is_attention=False,\n",
    "                                limit=500,\n",
    "                                load_if_exists=load_if_exists,\n",
    "                                models_dir=models_dir)\n",
    "    print('\\nExtracted Features:{}'.format(len(features)))\n",
    "\n",
    "    # load & clean descriptions\n",
    "    print(\"Extracting Captions:\")\n",
    "    captions = extract_captions(features, load_if_exists=load_if_exists)\n",
    "    print('\\nExtracted Captions:{}'.format(len(captions)))\n",
    "\n",
    "    # summarize vocabulary\n",
    "    vocabulary = extract_vocabulary(captions)\n",
    "    print('Vocabulary Size: %d' % len(vocabulary))\n",
    "\n",
    "    features_train, features_test = train_test_split(features)\n",
    "    captions_train_dict, captions_test_dict = train_test_split(captions)\n",
    "\n",
    "    print(\"features training:{}, features testing:{}\".format(len(features_train), len(features_test)))\n",
    "    print(\"captions training dict:{}, captions testing dict:{}\".format(len(captions_train_dict), len(captions_test_dict)))\n",
    "\n",
    "    # captions_train = load_clean_captions('models/captions.txt', features_train)\n",
    "    # captions_test = load_clean_captions('models/captions.txt', features_test)\n",
    "    captions_train = prepare_captions(captions_train_dict)\n",
    "    captions_test = prepare_captions(captions_test_dict)\n",
    "    print(\"captions training:{}, captions testing:{}\".format(len(captions_train), len(captions_test)))\n",
    "\n",
    "    verify_datasets(features_train, captions_train)\n",
    "    verify_datasets(features_test, captions_test)\n",
    "\n",
    "    return (features_train, captions_train), (features_test, captions_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_test(models_dir='/models'):\n",
    "    ml_type = 'test'\n",
    "    coco_train_full_path = dataDir + '/' + ml_type + year + '/'\n",
    "    return prepare_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting Features:\n",
      "Annotations of train captions Dataset: /data/annotations/captions_train2014.json\n",
      "loading annotations into memory...\n",
      "Done (t=0.52s)\n",
      "creating index...\n",
      "index created!\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_4 (InputLayer)         (None, 224, 224, 3)       0         \n",
      "_________________________________________________________________\n",
      "block1_conv1 (Conv2D)        (None, 224, 224, 64)      1792      \n",
      "_________________________________________________________________\n",
      "block1_conv2 (Conv2D)        (None, 224, 224, 64)      36928     \n",
      "_________________________________________________________________\n",
      "block1_pool (MaxPooling2D)   (None, 112, 112, 64)      0         \n",
      "_________________________________________________________________\n",
      "block2_conv1 (Conv2D)        (None, 112, 112, 128)     73856     \n",
      "_________________________________________________________________\n",
      "block2_conv2 (Conv2D)        (None, 112, 112, 128)     147584    \n",
      "_________________________________________________________________\n",
      "block2_pool (MaxPooling2D)   (None, 56, 56, 128)       0         \n",
      "_________________________________________________________________\n",
      "block3_conv1 (Conv2D)        (None, 56, 56, 256)       295168    \n",
      "_________________________________________________________________\n",
      "block3_conv2 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_conv3 (Conv2D)        (None, 56, 56, 256)       590080    \n",
      "_________________________________________________________________\n",
      "block3_pool (MaxPooling2D)   (None, 28, 28, 256)       0         \n",
      "_________________________________________________________________\n",
      "block4_conv1 (Conv2D)        (None, 28, 28, 512)       1180160   \n",
      "_________________________________________________________________\n",
      "block4_conv2 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_conv3 (Conv2D)        (None, 28, 28, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block4_pool (MaxPooling2D)   (None, 14, 14, 512)       0         \n",
      "_________________________________________________________________\n",
      "block5_conv1 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv2 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_conv3 (Conv2D)        (None, 14, 14, 512)       2359808   \n",
      "_________________________________________________________________\n",
      "block5_pool (MaxPooling2D)   (None, 7, 7, 512)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 25088)             0         \n",
      "_________________________________________________________________\n",
      "fc1 (Dense)                  (None, 4096)              102764544 \n",
      "_________________________________________________________________\n",
      "fc2 (Dense)                  (None, 4096)              16781312  \n",
      "=================================================================\n",
      "Total params: 134,260,544\n",
      "Trainable params: 134,260,544\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'module' object is not callable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-64-216f597dc343>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mprepare_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels_dir\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/data/models/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-62-ba7deae9d3dd>\u001b[0m in \u001b[0;36mprepare_data\u001b[0;34m(models_dir)\u001b[0m\n\u001b[1;32m      6\u001b[0m                                 \u001b[0mlimit\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m                                 \u001b[0mload_if_exists\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mload_if_exists\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m                                 models_dir=models_dir)\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'\\nExtracted Features:{}'\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-52-5568ae886cfb>\u001b[0m in \u001b[0;36mextract_features\u001b[0;34m(dataset_dir, is_attention, limit, load_if_exists, models_dir)\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mis_attention\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m     \u001b[0;31m# features = get_features_from_directory(dataset_dir, model, limit)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m     \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_features_from_coco\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcoco\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset_dir\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlimit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m     \u001b[0;31m# save to file\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m     \u001b[0msave_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'models/'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-20-6f18e7e669d0>\u001b[0m in \u001b[0;36mget_features_from_coco\u001b[0;34m(coco, directory, model, limit)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;31m# counter = 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mall_img_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"file_name\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcoco\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0;32mfor\u001b[0m \u001b[0mimg_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mprogressbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlimit\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlimit\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_img_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mall_img_ids\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mfilename\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mload_image\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'module' object is not callable"
     ]
    }
   ],
   "source": [
    "prepare_data(models_dir='/data/models/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
